2017-06-23 17:05:06.183489: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-06-23 17:05:06.183527: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-06-23 17:05:06.183535: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-06-23 17:05:06.183540: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-06-23 17:05:06.183545: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
:::Training Start:::
the step 1.0 takes 96.0890758038 loss 8.74610424042
===============Eval a batch=======================
the step 1.0 test accuracy: 0.0
===============Eval a batch=======================
Save the ckpt of 1.0
the step 2.0 takes 8.95507407188 loss 8.46141624451
the step 3.0 takes 8.96335792542 loss 8.33611679077
the step 4.0 takes 8.79552817345 loss 8.2946023941
the step 5.0 takes 8.99757695198 loss 8.36956310272
the step 6.0 takes 8.67174983025 loss 8.26969909668
the step 7.0 takes 8.78126096725 loss 8.34627151489
the step 8.0 takes 8.70847201347 loss 8.38667678833
the step 9.0 takes 8.82662796974 loss 8.28620815277
the step 10.0 takes 9.13169193268 loss 8.38661003113
the step 11.0 takes 8.94046187401 loss 8.42539596558
the step 12.0 takes 8.76144504547 loss 8.32880115509
the step 13.0 takes 8.97326493263 loss 8.3804101944
the step 14.0 takes 8.94849300385 loss 8.45573806763
the step 15.0 takes 8.92085504532 loss 8.34913063049
the step 16.0 takes 8.86816501617 loss 8.32188796997
the step 17.0 takes 9.00797891617 loss 8.44177341461
the step 18.0 takes 8.65694904327 loss 8.39462089539
the step 19.0 takes 8.76365399361 loss 8.33572006226
the step 20.0 takes 8.95333099365 loss 8.38325309753
the step 21.0 takes 8.99897503853 loss 8.38203144073
the step 22.0 takes 8.76748394966 loss 8.317943573
the step 23.0 takes 8.73736000061 loss 8.38072109222
the step 24.0 takes 8.77081108093 loss 8.32518768311
the step 25.0 takes 8.6756541729 loss 8.33011817932
the step 26.0 takes 8.65574193001 loss 8.29462337494
the step 27.0 takes 8.62157607079 loss 8.39948940277
the step 28.0 takes 8.48785114288 loss 8.29305267334
the step 29.0 takes 8.52483391762 loss 8.37263774872
the step 30.0 takes 8.56181001663 loss 8.35850143433
the step 31.0 takes 8.56739401817 loss 8.39378261566
the step 32.0 takes 8.50393390656 loss 8.35521411896
the step 33.0 takes 8.42639279366 loss 8.35054683685
the step 34.0 takes 8.47460198402 loss 8.40847396851
the step 35.0 takes 8.36234998703 loss 8.33673095703
the step 36.0 takes 8.51021695137 loss 8.34552383423
the step 37.0 takes 8.48436689377 loss 8.33378314972
the step 38.0 takes 8.61758780479 loss 8.43304443359
the step 39.0 takes 8.39678192139 loss 8.43495559692
the step 40.0 takes 8.3778398037 loss 8.29753303528
the step 41.0 takes 8.51794791222 loss 8.46074581146
the step 42.0 takes 8.18967604637 loss 8.39884662628
the step 43.0 takes 8.35886907578 loss 8.3249206543
the step 44.0 takes 8.88169813156 loss 8.39499664307
the step 45.0 takes 8.99242305756 loss 8.31644821167
the step 46.0 takes 8.95933318138 loss 8.3781785965
the step 47.0 takes 8.82217597961 loss 8.37001228333
the step 48.0 takes 8.99054193497 loss 8.40292549133
the step 49.0 takes 9.01423096657 loss 8.36458778381
the step 50.0 takes 8.70671105385 loss 8.36131572723
the step 51.0 takes 8.58946681023 loss 8.42310142517
the step 52.0 takes 8.97380113602 loss 8.5024356842
the step 53.0 takes 8.88593912125 loss 8.37286567688
the step 54.0 takes 8.98000407219 loss 8.30068588257
the step 55.0 takes 9.03428697586 loss 8.33109855652
the step 56.0 takes 8.84535908699 loss 8.40855979919
the step 57.0 takes 8.81744599342 loss 8.26073455811
the step 58.0 takes 8.9532790184 loss 8.22162628174
the step 59.0 takes 8.655616045 loss 8.32520294189
the step 60.0 takes 8.76054596901 loss 8.30114746094
the step 61.0 takes 8.82082104683 loss 8.30025863647
the step 62.0 takes 8.84283900261 loss 8.27110099792
the step 63.0 takes 8.9073908329 loss 8.31944942474
the step 64.0 takes 8.83839988708 loss 8.25314998627
the step 65.0 takes 8.78160500526 loss 8.21790313721
the step 66.0 takes 8.81966805458 loss 8.24447631836
the step 67.0 takes 9.00240898132 loss 8.19618034363
the step 68.0 takes 8.99254083633 loss 8.37595367432
the step 69.0 takes 8.78712415695 loss 8.29508399963
the step 70.0 takes 8.82994198799 loss 8.19259929657
the step 71.0 takes 8.88118290901 loss 8.2364025116
the step 72.0 takes 9.08303880692 loss 8.21750450134
the step 73.0 takes 8.97520899773 loss 8.19605636597
the step 74.0 takes 8.84452414513 loss 8.2497177124
the step 75.0 takes 9.22635793686 loss 8.22587299347
the step 76.0 takes 8.92366695404 loss 8.17656135559
the step 77.0 takes 8.92740702629 loss 8.14303016663
the step 78.0 takes 8.95627713203 loss 8.12269020081
the step 79.0 takes 9.02065896988 loss 8.2961397171
the step 80.0 takes 8.77757787704 loss 8.14839363098
the step 81.0 takes 9.1069021225 loss 8.24523162842
the step 82.0 takes 8.85936903954 loss 8.17743110657
the step 83.0 takes 8.84284901619 loss 8.15818309784
the step 84.0 takes 8.93189692497 loss 8.06304168701
the step 85.0 takes 8.57831597328 loss 8.22854804993
the step 86.0 takes 8.82991290092 loss 8.16444396973
the step 87.0 takes 9.12215089798 loss 8.08395862579
the step 88.0 takes 8.98046803474 loss 8.30119895935
the step 89.0 takes 8.69241595268 loss 8.0933971405
the step 90.0 takes 8.93341207504 loss 8.09501266479
the step 91.0 takes 8.91778111458 loss 8.00883579254
the step 92.0 takes 8.78466486931 loss 8.10910606384
the step 93.0 takes 8.77639698982 loss 8.01848220825
the step 94.0 takes 8.68666887283 loss 8.01785087585
the step 95.0 takes 8.74024200439 loss 8.08531761169
the step 96.0 takes 8.78822207451 loss 8.08893585205
the step 97.0 takes 8.75248599052 loss 8.12400627136
the step 98.0 takes 8.92836308479 loss 8.08588123322
the step 99.0 takes 9.13007807732 loss 8.0510597229
the step 100.0 takes 8.8600320816 loss 8.01880741119
the step 101.0 takes 8.90233898163 loss 8.08814048767
===============Eval a batch=======================
the step 101.0 test accuracy: 0.0
===============Eval a batch=======================
the step 102.0 takes 8.75689911842 loss 8.09052276611
the step 103.0 takes 8.83483409882 loss 8.01380634308
the step 104.0 takes 8.88249611855 loss 8.08021259308
the step 105.0 takes 8.95834803581 loss 7.97399997711
the step 106.0 takes 9.10368084908 loss 8.04066848755
the step 107.0 takes 9.03035283089 loss 7.9436750412
the step 108.0 takes 9.29218697548 loss 7.9103307724
the step 109.0 takes 9.08474087715 loss 7.99633741379
the step 110.0 takes 8.89199495316 loss 7.90516424179
the step 111.0 takes 8.73940300941 loss 7.94977378845
the step 112.0 takes 8.67819809914 loss 7.8857665062
the step 113.0 takes 8.94977402687 loss 7.95579910278
the step 114.0 takes 8.58882522583 loss 8.02278900146
the step 115.0 takes 8.97980999947 loss 7.8664855957
the step 116.0 takes 9.24842691422 loss 7.83423042297
the step 117.0 takes 8.85951709747 loss 7.76962184906
the step 118.0 takes 8.81873202324 loss 7.77986621857
the step 119.0 takes 8.95511102676 loss 7.70422077179
the step 120.0 takes 8.70862722397 loss 7.84089565277
the step 121.0 takes 8.83858084679 loss 7.85524272919
the step 122.0 takes 8.72058105469 loss 7.69337511063
the step 123.0 takes 8.93793797493 loss 7.86237621307
the step 124.0 takes 8.67412900925 loss 7.7105588913
