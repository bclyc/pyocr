2017-06-26 18:59:02.463715: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-06-26 18:59:02.463755: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-06-26 18:59:02.463761: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-06-26 18:59:02.463766: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-06-26 18:59:02.463770: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
trainsize:1023576,testsize:111282
:::Training Start:::
the step 1.0 takes 85.9981210232 loss 8.66744804382
===============Eval a batch=======================
the step 1.0 test accuracy: 0.0
===============Eval a batch=======================
Save the ckpt of 1.0
the step 2.0 takes 8.45159482956 loss 8.39610099792
the step 3.0 takes 8.24434995651 loss 8.3384513855
the step 4.0 takes 7.68730187416 loss 8.38515949249
the step 5.0 takes 7.77631688118 loss 8.36076164246
the step 6.0 takes 7.96757793427 loss 8.42507266998
the step 7.0 takes 7.66243100166 loss 8.46970748901
the step 8.0 takes 7.65175485611 loss 8.32613182068
the step 9.0 takes 7.73861598969 loss 8.40478134155
the step 10.0 takes 7.94008684158 loss 8.3848028183
the step 11.0 takes 7.90224599838 loss 8.51096153259
the step 12.0 takes 7.72310996056 loss 8.47794818878
the step 13.0 takes 7.63904500008 loss 8.52727127075
the step 14.0 takes 7.67624187469 loss 8.53144073486
the step 15.0 takes 8.05362200737 loss 8.4113035202
the step 16.0 takes 7.69621109962 loss 8.49427604675
the step 17.0 takes 8.04401111603 loss 8.56245517731
the step 18.0 takes 7.97375297546 loss 8.59356880188
the step 19.0 takes 7.78125500679 loss 8.55808734894
the step 20.0 takes 7.67843604088 loss 8.42135429382
the step 21.0 takes 7.82854104042 loss 8.52594184875
the step 22.0 takes 7.70465707779 loss 8.54580593109
the step 23.0 takes 7.84585189819 loss 8.49763679504
the step 24.0 takes 7.73713612556 loss 8.63447666168
the step 25.0 takes 7.73835611343 loss 8.52067947388
the step 26.0 takes 7.98344993591 loss 8.53561210632
the step 27.0 takes 7.80342793465 loss 8.53648471832
the step 28.0 takes 7.98137497902 loss 8.66770744324
the step 29.0 takes 7.91637921333 loss 8.52533912659
the step 30.0 takes 7.71155595779 loss 8.61292457581
the step 31.0 takes 7.71757006645 loss 8.44981479645
the step 32.0 takes 8.30667805672 loss 8.67597866058
the step 33.0 takes 8.68221783638 loss 8.43341255188
the step 34.0 takes 8.04013490677 loss 8.56764125824
the step 35.0 takes 7.7445499897 loss 8.49368286133
the step 36.0 takes 7.62359404564 loss 8.35966968536
the step 37.0 takes 7.72163105011 loss 8.45449733734
the step 38.0 takes 7.87496495247 loss 8.50988578796
the step 39.0 takes 7.73263120651 loss 8.62275123596
the step 40.0 takes 7.89792704582 loss 8.44824886322
the step 41.0 takes 7.74687719345 loss 8.5528793335
the step 42.0 takes 7.81170320511 loss 8.50982093811
the step 43.0 takes 7.65548205376 loss 8.48763275146
the step 44.0 takes 7.64367890358 loss 8.39211273193
the step 45.0 takes 7.80912899971 loss 8.32881259918
the step 46.0 takes 7.63801908493 loss 8.39881801605
the step 47.0 takes 7.72433185577 loss 8.48905754089
the step 48.0 takes 7.65999889374 loss 8.5504360199
the step 49.0 takes 7.58800506592 loss 8.40780735016
the step 50.0 takes 7.69147014618 loss 8.50913906097
the step 51.0 takes 7.65322709084 loss 8.5183801651
the step 52.0 takes 7.78017807007 loss 8.36519813538
the step 53.0 takes 7.79435181618 loss 8.41395568848
the step 54.0 takes 7.60961985588 loss 8.44674682617
the step 55.0 takes 7.73261213303 loss 8.43243408203
the step 56.0 takes 7.78081583977 loss 8.52216053009
the step 57.0 takes 7.63980698586 loss 8.34798526764
the step 58.0 takes 7.69343495369 loss 8.40605831146
the step 59.0 takes 7.54978299141 loss 8.43468475342
the step 60.0 takes 7.67249083519 loss 8.47125053406
the step 61.0 takes 7.9180150032 loss 8.43207359314
the step 62.0 takes 7.82501101494 loss 8.43146133423
the step 63.0 takes 7.75505781174 loss 8.5606842041
the step 64.0 takes 7.92966389656 loss 8.42198371887
the step 65.0 takes 7.85589313507 loss 8.4148349762
the step 66.0 takes 7.50537705421 loss 8.48431587219
the step 67.0 takes 7.71694588661 loss 8.38349437714
the step 68.0 takes 7.52521586418 loss 8.50653553009
the step 69.0 takes 7.71499109268 loss 8.47571277618
the step 70.0 takes 7.67033195496 loss 8.42626476288
the step 71.0 takes 7.83072304726 loss 8.45532989502
the step 72.0 takes 7.85849905014 loss 8.38872432709
the step 73.0 takes 7.6135020256 loss 8.36421966553
the step 74.0 takes 7.56769990921 loss 8.39884757996
the step 75.0 takes 7.5513420105 loss 8.30335903168
the step 76.0 takes 7.87779188156 loss 8.49151611328
the step 77.0 takes 7.65099906921 loss 8.34241485596
the step 78.0 takes 7.85907196999 loss 8.44088554382
the step 79.0 takes 7.77434206009 loss 8.33123779297
the step 80.0 takes 7.704226017 loss 8.40013504028
the step 81.0 takes 7.6929500103 loss 8.46519851685
the step 82.0 takes 7.91045093536 loss 8.38766288757
the step 83.0 takes 7.78702497482 loss 8.26551818848
the step 84.0 takes 7.86417293549 loss 8.44460868835
the step 85.0 takes 7.77004694939 loss 8.45884609222
the step 86.0 takes 7.61635899544 loss 8.40184497833
the step 87.0 takes 7.73189878464 loss 8.35627555847
the step 88.0 takes 7.83623504639 loss 8.33416461945
the step 89.0 takes 7.70630478859 loss 8.30964946747
